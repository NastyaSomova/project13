{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sB0dbhb5qGGs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm\n",
        "from torchvision.utils import save_image, make_grid"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of nn model"
      ],
      "metadata": {
        "id": "YcKy1eDnrhVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "        self._input = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.LeakyReLU(0.2),\n",
        "        )\n",
        "        self._fc_mean  = nn.Linear(hidden_dim, latent_dim)\n",
        "        self._fc_var   = nn.Linear (hidden_dim, latent_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output_  = self._input(x)\n",
        "        mean_    = self._fc_mean(output_)\n",
        "        log_var_ = self._fc_var(output_)\n",
        "        return mean_, log_var_\n",
        "\n",
        "# Model Hyperparameters\n",
        "dataset_path = '~/datasets'\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "batch_size = 100\n",
        "x_dim  = 784\n",
        "hidden_dim = 400\n",
        "latent_dim = 200\n",
        "lr = 1e-3\n",
        "epochs = 30\n",
        "\n",
        "model = Encoder(input_dim=x_dim, hidden_dim=hidden_dim, latent_dim=latent_dim)"
      ],
      "metadata": {
        "id": "hLCTVvAsqXfJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6-6mTB64rLqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torchviz code for extracting model structure"
      ],
      "metadata": {
        "id": "cwQ7PkyZrLwv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import namedtuple\n",
        "from distutils.version import LooseVersion\n",
        "from graphviz import Digraph\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import warnings\n",
        "\n",
        "Node = namedtuple('Node', ('name', 'inputs', 'attr', 'op'))\n",
        "\n",
        "# Saved attrs for grad_fn (incl. saved variables) begin with `._saved_*`\n",
        "SAVED_PREFIX = \"_saved_\"\n",
        "\n",
        "def get_fn_name(fn, show_attrs, max_attr_chars):\n",
        "    name = str(type(fn).__name__)\n",
        "    if not show_attrs:\n",
        "        return name\n",
        "    attrs = dict()\n",
        "    for attr in dir(fn):\n",
        "        if not attr.startswith(SAVED_PREFIX):\n",
        "            continue\n",
        "        val = getattr(fn, attr)\n",
        "        attr = attr[len(SAVED_PREFIX):]\n",
        "        if torch.is_tensor(val):\n",
        "            attrs[attr] = \"[saved tensor]\"\n",
        "        elif isinstance(val, tuple) and any(torch.is_tensor(t) for t in val):\n",
        "            attrs[attr] = \"[saved tensors]\"\n",
        "        else:\n",
        "            attrs[attr] = str(val)\n",
        "    if not attrs:\n",
        "        return name\n",
        "    max_attr_chars = max(max_attr_chars, 3)\n",
        "    col1width = max(len(k) for k in attrs.keys())\n",
        "    col2width = min(max(len(str(v)) for v in attrs.values()), max_attr_chars)\n",
        "    sep = \"-\" * max(col1width + col2width + 2, len(name))\n",
        "    attrstr = '%-' + str(col1width) + 's: %' + str(col2width)+ 's'\n",
        "    truncate = lambda s: s[:col2width - 3] + \"...\" if len(s) > col2width else s\n",
        "    params = '\\n'.join(attrstr % (k, truncate(str(v))) for (k, v) in attrs.items())\n",
        "    return name + '\\n' + sep + '\\n' + params\n",
        "\n",
        "def make_dot_from_trace(trace):\n",
        "    \"\"\" This functionality is not available in pytorch core at\n",
        "    https://pytorch.org/docs/stable/tensorboard.html\n",
        "    \"\"\"\n",
        "    # from tensorboardX\n",
        "    raise NotImplementedError(\"This function has been moved to pytorch core and \"\n",
        "                              \"can be found here: https://pytorch.org/docs/stable/tensorboard.html\")\n",
        "\n",
        "\n",
        "def resize_graph(dot, size_per_element=0.15, min_size=12):\n",
        "    \"\"\"Resize the graph according to how much content it contains.\n",
        "\n",
        "    Modify the graph in place.\n",
        "    \"\"\"\n",
        "    # Get the approximate number of nodes and edges\n",
        "    num_rows = len(dot.body)\n",
        "    content_size = num_rows * size_per_element\n",
        "    size = max(min_size, content_size)\n",
        "    size_str = str(size) + \",\" + str(size)\n",
        "    dot.graph_attr.update(size=size_str)\n",
        "\n",
        "def my_make_dot(var, params=None, show_attrs=False, show_saved=False, max_attr_chars=50):\n",
        "    \"\"\" Produces Graphviz representation of PyTorch autograd graph.\n",
        "\n",
        "    If a node represents a backward function, it is gray. Otherwise, the node\n",
        "    represents a tensor and is either blue, orange, or green:\n",
        "     - Blue: reachable leaf tensors that requires grad (tensors whose .grad\n",
        "         fields will be populated during .backward())\n",
        "     - Orange: saved tensors of custom autograd functions as well as those\n",
        "         saved by built-in backward nodes\n",
        "     - Green: tensor passed in as outputs\n",
        "     - Dark green: if any output is a view, we represent its base tensor with\n",
        "         a dark green node.\n",
        "\n",
        "    Args:\n",
        "        var: output tensor\n",
        "        params: dict of (name, tensor) to add names to node that requires grad\n",
        "        show_attrs: whether to display non-tensor attributes of backward nodes\n",
        "            (Requires PyTorch version >= 1.9)\n",
        "        show_saved: whether to display saved tensor nodes that are not by custom\n",
        "            autograd functions. Saved tensor nodes for custom functions, if\n",
        "            present, are always displayed. (Requires PyTorch version >= 1.9)\n",
        "        max_attr_chars: if show_attrs is True, sets max number of characters\n",
        "            to display for any given attribute.\n",
        "    \"\"\"\n",
        "\n",
        "    if params is not None:\n",
        "        assert all(isinstance(p, Variable) for p in params.values())\n",
        "        param_map = {id(v): k for k, v in params.items()}\n",
        "    else:\n",
        "        param_map = {}\n",
        "\n",
        "    node_attr = dict(style='filled',\n",
        "                     shape='box',\n",
        "                     align='left',\n",
        "                     fontsize='10',\n",
        "                     ranksep='0.1',\n",
        "                     height='0.2',\n",
        "                     fontname='monospace')\n",
        "    dot = Digraph(node_attr=node_attr, graph_attr=dict(size=\"12,12\"))\n",
        "    seen = set()\n",
        "\n",
        "    def size_to_str(size):\n",
        "        return '(' + (', ').join(['%d' % v for v in size]) + ')'\n",
        "\n",
        "    def get_var_name(var, name=None):\n",
        "        if not name:\n",
        "            name = param_map[id(var)] if id(var) in param_map else ''\n",
        "        return '%s\\n %s' % (name, size_to_str(var.size()))\n",
        "\n",
        "    def add_nodes(fn):\n",
        "        assert not torch.is_tensor(fn)\n",
        "        if fn in seen:\n",
        "            return\n",
        "        seen.add(fn)\n",
        "\n",
        "        if show_saved:\n",
        "            for attr in dir(fn):\n",
        "                if not attr.startswith(SAVED_PREFIX):\n",
        "                    continue\n",
        "                val = getattr(fn, attr)\n",
        "                seen.add(val)\n",
        "                attr = attr[len(SAVED_PREFIX):]\n",
        "                if torch.is_tensor(val):\n",
        "                    dot.edge(str(id(fn)), str(id(val)), dir=\"none\")\n",
        "                    dot.node(str(id(val)), get_var_name(val, attr), fillcolor='orange')\n",
        "                if isinstance(val, tuple):\n",
        "                    for i, t in enumerate(val):\n",
        "                        if torch.is_tensor(t):\n",
        "                            name = attr + '[%s]' % str(i)\n",
        "                            dot.edge(str(id(fn)), str(id(t)), dir=\"none\")\n",
        "                            dot.node(str(id(t)), get_var_name(t, name), fillcolor='orange')\n",
        "\n",
        "        if hasattr(fn, 'variable'):\n",
        "            # if grad_accumulator, add the node for .variable\n",
        "            var = fn.variable\n",
        "            seen.add(var)\n",
        "            dot.node(str(id(var)), get_var_name(var), fillcolor='lightblue')\n",
        "            dot.edge(str(id(var)), str(id(fn)))\n",
        "\n",
        "        # add the node for this grad_fn\n",
        "        dot.node(str(id(fn)), get_fn_name(fn, show_attrs, max_attr_chars))\n",
        "\n",
        "        # recurse\n",
        "        if hasattr(fn, 'next_functions'):\n",
        "            for u in fn.next_functions:\n",
        "                if u[0] is not None:\n",
        "                    dot.edge(str(id(u[0])), str(id(fn)))\n",
        "                    add_nodes(u[0])\n",
        "\n",
        "# note: this used to show .saved_tensors in pytorch0.2, but stopped\n",
        "        # working* as it was moved to ATen and Variable-Tensor merged\n",
        "        # also note that this still works for custom autograd functions\n",
        "        if hasattr(fn, 'saved_tensors'):\n",
        "            for t in fn.saved_tensors:\n",
        "                seen.add(t)\n",
        "                dot.edge(str(id(t)), str(id(fn)), dir=\"none\")\n",
        "                dot.node(str(id(t)), get_var_name(t), fillcolor='orange')\n",
        "\n",
        "\n",
        "    def add_base_tensor(var, color='darkolivegreen1'):\n",
        "        if var in seen:\n",
        "            return\n",
        "        seen.add(var)\n",
        "        dot.node(str(id(var)), get_var_name(var), fillcolor=color)\n",
        "        if (var.grad_fn):\n",
        "            add_nodes(var.grad_fn)\n",
        "            dot.edge(str(id(var.grad_fn)), str(id(var)))\n",
        "        if var._is_view():\n",
        "            add_base_tensor(var._base, color='darkolivegreen3')\n",
        "            dot.edge(str(id(var._base)), str(id(var)), style=\"dotted\")\n",
        "\n",
        "\n",
        "    # handle multiple outputs\n",
        "    if isinstance(var, tuple):\n",
        "        for v in var:\n",
        "            add_base_tensor(v)\n",
        "    else:\n",
        "        add_base_tensor(var)\n",
        "\n",
        "    resize_graph(dot)\n",
        "\n",
        "    return dot"
      ],
      "metadata": {
        "id": "jQB2Kiheq1PB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "\n",
        "def my_make_json(var, params=None, show_attrs=False, show_saved=False, max_attr_chars=50):\n",
        "    \"\"\"Преобразует граф вычислений PyTorch в JSON-совместимый словарь.\"\"\"\n",
        "\n",
        "    if params is not None:\n",
        "        assert all(isinstance(p, Variable) for p in params.values())\n",
        "        param_map = {id(v): k for k, v in params.items()}\n",
        "    else:\n",
        "        param_map = {}\n",
        "\n",
        "    graph_data = {\"nodes\": [], \"edges\": []}\n",
        "    seen = set()\n",
        "\n",
        "    def size_to_str(size):\n",
        "        return '(' + (', ').join(['%d' % v for v in size]) + ')'\n",
        "\n",
        "    def get_var_name(var, name=None):\n",
        "        if not name:\n",
        "            name = param_map[id(var)] if id(var) in param_map else ''\n",
        "        return f'{name} {size_to_str(var.size())}'\n",
        "\n",
        "    def get_fn_name(fn):\n",
        "        \"\"\"Возвращает имя функции в графе\"\"\"\n",
        "        return str(type(fn).__name__)\n",
        "\n",
        "    def add_nodes(fn):\n",
        "        \"\"\"Рекурсивно добавляет узлы в словарь\"\"\"\n",
        "        if fn in seen:\n",
        "            return\n",
        "        seen.add(fn)\n",
        "\n",
        "        # Добавляем текущий узел\n",
        "        graph_data[\"nodes\"].append({\n",
        "            \"id\": id(fn),\n",
        "            \"type\": \"Function\",\n",
        "            \"name\": get_fn_name(fn)\n",
        "        })\n",
        "\n",
        "        # Добавляем связи\n",
        "        if hasattr(fn, 'next_functions'):\n",
        "            for u in fn.next_functions:\n",
        "                if u[0] is not None:\n",
        "                    graph_data[\"edges\"].append({\"from\": id(u[0]), \"to\": id(fn)})\n",
        "                    add_nodes(u[0])\n",
        "\n",
        "        # Обрабатываем сохраненные тензоры\n",
        "        if show_saved and hasattr(fn, 'saved_tensors'):\n",
        "            for t in fn.saved_tensors:\n",
        "                if t not in seen:\n",
        "                    seen.add(t)\n",
        "                    graph_data[\"nodes\"].append({\n",
        "                        \"id\": id(t),\n",
        "                        \"type\": \"Tensor\",\n",
        "                        \"name\": get_var_name(t),\n",
        "                        \"color\": \"orange\"\n",
        "                    })\n",
        "                    graph_data[\"edges\"].append({\"from\": id(t), \"to\": id(fn)})\n",
        "\n",
        "    def add_base_tensor(var, color='lightblue'):\n",
        "        \"\"\"Добавляет начальный тензор в граф\"\"\"\n",
        "        if var in seen:\n",
        "            return\n",
        "        seen.add(var)\n",
        "\n",
        "        graph_data[\"nodes\"].append({\n",
        "            \"id\": id(var),\n",
        "            \"type\": \"Tensor\",\n",
        "            \"name\": get_var_name(var),\n",
        "            \"color\": color\n",
        "        })\n",
        "\n",
        "        if var.grad_fn:\n",
        "            add_nodes(var.grad_fn)\n",
        "            graph_data[\"edges\"].append({\"from\": id(var.grad_fn), \"to\": id(var)})\n",
        "\n",
        "        if var._is_view():\n",
        "            add_base_tensor(var._base, color='darkolivegreen3')\n",
        "            graph_data[\"edges\"].append({\"from\": id(var._base), \"to\": id(var), \"style\": \"dotted\"})\n",
        "\n",
        "    # Обрабатываем входной тензор (или кортеж)\n",
        "    if isinstance(var, tuple):\n",
        "        for v in var:\n",
        "            add_base_tensor(v)\n",
        "    else:\n",
        "        add_base_tensor(var)\n",
        "\n",
        "    return graph_data\n",
        "\n",
        "# Пример использования\n",
        "x = torch.randn(1, 10, requires_grad=True)\n",
        "y = torch.randn(10, 5, requires_grad=True)\n",
        "z = torch.matmul(x, y)  # Пример вычисления\n",
        "\n"
      ],
      "metadata": {
        "id": "WNTfVOFKvRt7"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc = Encoder(10, 12, 4)\n",
        "x = np.random.normal(1, size=10)\n",
        "t = torch.tensor(x, dtype=torch.float32).reshape(1, 10)\n",
        "out = enc(t)\n",
        "my_make_dot(out, params=dict(enc.named_parameters())).render(\"my_model_graph\", format=\"png\")\n",
        "\n",
        "graph_dict = my_make_json(z)\n",
        "\n",
        "import json\n",
        "print(json.dumps(graph_dict, indent=2))  # Вывод в формате JSON"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-mLNU0r6rIEW",
        "outputId": "7a1cfe4b-f7c2-4d15-bc37-65f40ae44007"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"nodes\": [\n",
            "    {\n",
            "      \"id\": 133701896272784,\n",
            "      \"type\": \"Tensor\",\n",
            "      \"name\": \" (1, 5)\",\n",
            "      \"color\": \"lightblue\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": 133702047040608,\n",
            "      \"type\": \"Function\",\n",
            "      \"name\": \"MmBackward0\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": 133702047045168,\n",
            "      \"type\": \"Function\",\n",
            "      \"name\": \"AccumulateGrad\"\n",
            "    },\n",
            "    {\n",
            "      \"id\": 133702047042816,\n",
            "      \"type\": \"Function\",\n",
            "      \"name\": \"AccumulateGrad\"\n",
            "    }\n",
            "  ],\n",
            "  \"edges\": [\n",
            "    {\n",
            "      \"from\": 133702047045168,\n",
            "      \"to\": 133702047040608\n",
            "    },\n",
            "    {\n",
            "      \"from\": 133702047042816,\n",
            "      \"to\": 133702047040608\n",
            "    },\n",
            "    {\n",
            "      \"from\": 133702047040608,\n",
            "      \"to\": 133701896272784\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hymuhzsXr_HL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extracting weight and bias"
      ],
      "metadata": {
        "id": "NI6NFsnYsHoy"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_layer(model, path):\n",
        "    parts = path.split(\".\")\n",
        "    layer = model\n",
        "    for part in parts:\n",
        "        if part.isdigit():\n",
        "            part = int(part)\n",
        "            layer =layer[part]\n",
        "        else:\n",
        "          layer = getattr(layer, part)\n",
        "    return layer"
      ],
      "metadata": {
        "id": "l2ln1QVasLqn"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_weight(model, path):\n",
        "  layer = get_layer(model, path)\n",
        "  return layer.state_dict()['weight']\n",
        "\n",
        "def get_bias(model, path):\n",
        "  layer = get_layer(model, path)\n",
        "  return layer.state_dict()['bias']"
      ],
      "metadata": {
        "id": "yfG3GwVksNYN"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_weight(model, '_input.0')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXoqU3i2sPGE",
        "outputId": "ed2661c6-4eea-4a8f-8675-ed29dd41393d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0272,  0.0201, -0.0155,  ..., -0.0011, -0.0295,  0.0244],\n",
              "        [-0.0219,  0.0068, -0.0195,  ..., -0.0135, -0.0032, -0.0219],\n",
              "        [ 0.0121,  0.0173, -0.0285,  ..., -0.0311,  0.0163, -0.0347],\n",
              "        ...,\n",
              "        [ 0.0207, -0.0115, -0.0011,  ..., -0.0333, -0.0096,  0.0034],\n",
              "        [-0.0289,  0.0167, -0.0317,  ..., -0.0135,  0.0310, -0.0085],\n",
              "        [ 0.0357,  0.0221,  0.0320,  ..., -0.0097, -0.0074, -0.0180]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_6wc3I7xsURP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}